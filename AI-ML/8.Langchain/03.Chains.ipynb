{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9756d9b-5256-4687-8df4-e26938b52cc4",
   "metadata": {},
   "source": [
    "## Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cacf131-bd2d-4dd8-aa73-0cc799ec2ad4",
   "metadata": {},
   "source": [
    "Chains are building blocks of Langchain. With langchain we can chain multiple components to be ran in sequence. This way we don't have to worry about logic to get output forn one component and send it to other component as we did with Prompts and LLM previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda92c9d-de4c-4b69-914b-8f8a46141434",
   "metadata": {},
   "source": [
    "#### Setup LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9937d3ab-4f8a-4be6-ae0e-85ecb63cfbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "# Create chat ollama class to interact with ollama models\n",
    "llm = ChatOllama(\n",
    "    base_url = 'http://localhost:11434',\n",
    "    model = 'qwen2.5:0.5b'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553f0094-d737-4b23-9884-fcb4438e7fa4",
   "metadata": {},
   "source": [
    "#### Create Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e38ed9de-1cec-4bc3-a962-5bc527ec4745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# This is simple Prompt template\n",
    "prompt = PromptTemplate(template = \"\"\"You are a super helpful health expert AI assistant. You can answer questions on health related topic.\n",
    "                                    Answer the health related questions.\n",
    "                                    If the question is not related to health, tell user that you can't answer this question.\n",
    "                                    Keep the answer concise and maximum 3 sentences.\n",
    "                                    Question: {question}\n",
    "                                    Answer: \"\"\",\n",
    "                       input_variables=['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17c74869-9b90-4757-922e-4bc690caf49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ask Question on Health:  What are steroids?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steroids are a type of drug used to treat various conditions including high blood pressure (hypertension), heart disease, diabetes, osteoporosis, and asthma.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "# LLM chain is simplest chain which takes prompt and LLM as input and chain them together\n",
    "chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# And now we can simply invoke the chain which will take care of building prompt and sending prompt to llm\n",
    "question = input(\"Ask Question on Health: \").strip().strip('\\n')\n",
    "answer = chain.invoke({'question': question})['text']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b90dea-b53d-47d8-b3a7-07d35d1c45df",
   "metadata": {},
   "source": [
    "### Utility Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f704758-3d1a-4e75-8e1a-0658dcaf9cfd",
   "metadata": {},
   "source": [
    "There are some prebuilt chains for specific utility function.\n",
    "For e.g. MathChain  \n",
    "    If you ask a complex math question to LLM, it is definitely going to fail. \n",
    "    So Math chain ask LLM to generate a python script to solve the math question and then executes the python logic to give accurate result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc549c12-0953-44b5-90af-1705043fa0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"calculate value of 225 raised to power 0.05\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e207e8c-89e1-453f-8ffd-989cc2872b5b",
   "metadata": {},
   "source": [
    "#### With Just LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0398702-b111-480a-90ed-23e0f1ff0218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, let's calculate the value of \\(225^{0.05}\\).\n",
      "\n",
      "First, we need to approximate the value:\n",
      "\\[\n",
      "225 \\approx 10^3\n",
      "\\]\n",
      "So,\n",
      "\\[\n",
      "225^{0.05} = (10^3)^{0.05}\n",
      "\\]\n",
      "\n",
      "Next, we use the property of exponents \\((a^m)^n = a^{mn}\\):\n",
      "\\[\n",
      "(10^3)^{0.05} = 10^{3 \\cdot 0.05} = 10^{0.15}\n",
      "\\]\n",
      "\n",
      "Now, let's approximate \\(10^{0.15}\\). We know that:\n",
      "\\[\n",
      "10^{0.15} \\approx 2\n",
      "\\]\n",
      "This is because \\(10^0 = 1\\) and raising \\(10\\) to a power between 0 and 1 gives a value close to 1 but slightly less than 1.\n",
      "\n",
      "Therefore, the approximate value of \\(225^{0.05}\\) is:\n",
      "\\[\n",
      "\\boxed{2}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "answer = llm.invoke(question).content\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e686ba3d-afc4-4dec-ad12-fa1ced5f9ff8",
   "metadata": {},
   "source": [
    "#### With Math Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12351c86-4178-4ac3-a56b-3693ebbf73f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'calculate value of 225 raised to power 0.05', 'answer': 'Answer: 1.31101942303975'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "chain = LLMMathChain.from_llm(llm=llm)\n",
    "answer = chain.invoke(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
