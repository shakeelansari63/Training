{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "Multilayer Perceptron is classic feed forward Artificial Neural Network.\n",
    "  \n",
    "MLP has some input nodes which are features which are inputted to multiple hidden layer nodes.  \n",
    "These hidden layer nodes have different functions and the outputted to output node which does prediction.\n",
    "\n",
    "![MLP Nodes](img/mlp_node.png)\n",
    "\n",
    "### MLP Classifier and Regressor\n",
    "Multi-layer Perceptron can be used as both regressor and classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron: Hyperparameters\n",
    "\n",
    "Import [`MLPClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) and [`MLPRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) from `sklearn` and explore the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Multilayer Perceptron Algorithm for Classification & Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "             power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "             tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "             warm_start=False) \n",
      "\n",
      " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "\n",
    "## See all the available HyperParameters\n",
    "print(MLPRegressor(),'\\n\\n', MLPClassifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major Hyperparameters for MLP\n",
    "Here you can see both MLPRegressor and MLPClassifier use same set of hyper-parameters.\n",
    "We will be tuning only 3 hyperparemeters here.\n",
    "\n",
    "### activation\n",
    "This is function which dictates the non-linearity in model.  \n",
    "Major activation functions are \n",
    "#### Sigmoid which is also called Logistic curve between 0 and 1\n",
    "#### TanH which is hyperbolic tangent  curve between -1 and 1\n",
    "#### ReLU (Rectified linear unit) this modified any value below 0 to 0 and traces the line\n",
    "\n",
    "![Activation Function](img/mlp_act.png)\n",
    "\n",
    "### hidden_layer_size\n",
    "This tell how many hidden layers are there and number of nodes in hidden layers.\n",
    "In above diagram, we had 1 hidden layer with 5 nodes. \n",
    "\n",
    "More hidden layers may lead to overfitting and training time complexity.\n",
    "\n",
    "### learning_rate\n",
    "This facilitate how quickly model find optimal solution.\n",
    "  \n",
    "The learning_rate hyperparameter tell how teh learning rate changes whereas the initial learning rate is defined by learning_rate_init. \n",
    "  \n",
    "Big Learning rate may miss and never find optimal solution\n",
    "Small learning rate will find optimal solution but may take time.\n",
    "![MLP Learning Rate](img/mlp_learn.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
