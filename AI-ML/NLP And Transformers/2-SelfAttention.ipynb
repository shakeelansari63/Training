{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/axel-sirota/nlp-and-transformers/blob/main/module3/NLPTransformers_Mod3Demo2_SelfAttention_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlMiC8Oof1f9"
   },
   "source": [
    "# Introducing self attention\n",
    "\n",
    "© Data Trainers LLC. GPL v 3.0.\n",
    "\n",
    "Author: Axel Sirota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOwkqwaFf5xC"
   },
   "source": [
    "In order to get closer to the Transformer we will neeed to understand Se;f Attention. this concept simply relates attention with a database. In a database you have a key value pair, and with a query you get a key and with that key you return the value, right?\n",
    "\n",
    "In self-attention ( or sometimes called  Q, K, V attention) we do the same, but instead of getting one key we will get:\n",
    "\n",
    "$$a_{i, k} = similarity(Q_i, K_k)$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$ c_i = ∑_{k}a_{i,k}*v_k $$\n",
    "\n",
    "Which basically means we get a weighted average of **ALL** the values for every input word we want to calculate the alignment. Therefore the term self attention. The diference with the previous attention is that before the similarity function was the dot product, and the matrices `K,Q,V` where the identity (only ones in the diagonal) and here they are learneable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPR2YRs5hyp7"
   },
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GTDmOLKh4gJy"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "from nltk.data import find\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"word2vec_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cc9Eevlj573a"
   },
   "outputs": [],
   "source": [
    "def softmax(x, axis=0):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bd99pIV059c5"
   },
   "outputs": [],
   "source": [
    "def get_word2vec_embedding(words):\n",
    "    \"\"\"\n",
    "    Function that takes in a list of words and returns a list of their embeddings,\n",
    "    based on a pretrained word2vec encoder.\n",
    "    \"\"\"\n",
    "    word2vec_sample = str(find(\"models/word2vec_sample/pruned.word2vec.txt\"))\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        word2vec_sample, binary=False\n",
    "    )\n",
    "\n",
    "    output = []\n",
    "    words_pass = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            output.append(np.array(model.word_vec(word)))\n",
    "            words_pass.append(word)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    embeddings = np.array(output)\n",
    "    del model  # free up space again\n",
    "    return embeddings, words_pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTZbCxr26Bgb"
   },
   "outputs": [],
   "source": [
    "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
    "    \"\"\"Function that takes in a weight matrix and plots it with custom axis ticks\"\"\"\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    ax = sns.heatmap(weight_matrix, cmap=\"Blues\")\n",
    "    plt.xticks(np.arange(weight_matrix.shape[1]) + 0.5, x_ticks)\n",
    "    plt.yticks(np.arange(weight_matrix.shape[0]) + 0.5, y_ticks)\n",
    "    plt.title(\"Attention matrix\")\n",
    "    plt.xlabel(\"Attention score\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJzGDX_d6EZb"
   },
   "outputs": [],
   "source": [
    "def embed_sentence(sentence):\n",
    "    # Embed a sentence using word2vec; for example use cases only.\n",
    "    sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence)\n",
    "    words = sentence.split()\n",
    "    word_vector_sequence, words = get_word2vec_embedding(words)\n",
    "    return np.expand_dims(word_vector_sequence, axis=0), words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9o-r0urHh1g7"
   },
   "source": [
    "## Seeing the attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9W-OoTWZiLhq"
   },
   "source": [
    "In order to see self attention, we just need to do what we did before! But now we will use a variant called **Scaled self attention** which is the one Transformers almost use:\n",
    "\n",
    "$\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQphfJ45i7-n"
   },
   "source": [
    "It is *very* important to remark all of this later will be done  by the Tensorflow or PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w70po97I8lC0"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    d_k = key.shape[-1]\n",
    "    logits = np.matmul(query, np.swapaxes(key, -2, -1))\n",
    "    scaled_logits = logits / np.sqrt(d_k)\n",
    "    attention_weights = softmax(scaled_logits, axis=-1)\n",
    "    value = np.matmul(attention_weights, value)\n",
    "    return value, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL8GwjiPjEnB"
   },
   "source": [
    "## Testing it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-kBX6ws8lM8"
   },
   "outputs": [],
   "source": [
    "sentence = \"I drink coke, but eat steak\"\n",
    "word_embeddings, words = embed_sentence(sentence)\n",
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pja4Rpzv8m1A"
   },
   "outputs": [],
   "source": [
    "Q = K = V = word_embeddings   # Para no entrenar\n",
    "\n",
    "# calculate weights and plot\n",
    "values, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "words = re.sub(r\"[^\\w\\s]\", \"\", sentence).split()\n",
    "plot_attention_weight_matrix(attention_weights[0], words, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_FoLZnejI1z"
   },
   "source": [
    "In effect you can see a positive alignment between drink and coke, as well as eat and steak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1VAN8j-68oRA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO4aN5wsYaRJLIklKNZA/t/",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
