{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Compare model results and final model selection\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "In this section, we will do the following:\n",
    "1. Evaluate all of our saved models on the validation set\n",
    "2. Select the best model based on performance on the validation set\n",
    "3. Evaluate that model on the holdout test set\n",
    "\n",
    "## Comparision of our models on different parameters\n",
    "![Model Comparision](img/model_comp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "val_features = pd.read_csv('tmp/val_features.csv')\n",
    "val_labels = pd.read_csv('tmp/val_labels.csv')\n",
    "\n",
    "te_features = pd.read_csv('tmp/test_features.csv')\n",
    "te_labels = pd.read_csv('tmp/test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "models = {}\n",
    "\n",
    "for mdl in ['LR', 'SVM', 'MLP', 'RF', 'GB']:\n",
    "    models[mdl] = joblib.load(f'joblib/{mdl}_Model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
      "                   warm_start=False) \n",
      "\n",
      "SVC(C=0.1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False) \n",
      "\n",
      "MLPClassifier(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False) \n",
      "\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=4, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=5,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False) \n",
      "\n",
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.01, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## See how models look\n",
    "for model in models:\n",
    "    print(models[model], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models on the validation set\n",
    "\n",
    "![Evaluation Metrics](img/eval_metrics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from time import time\n",
    "\n",
    "def evaluate_model(name, model, features, labels):\n",
    "    ## Start timer\n",
    "    start = time()\n",
    "    \n",
    "    ## Perform predition\n",
    "    pred = model.predict(features)\n",
    "    \n",
    "    ## End timer\n",
    "    end = time()\n",
    "    \n",
    "    ## Calculate Accuracy, Precision and Recall\n",
    "    accuracy = round(accuracy_score(labels, pred), 3)\n",
    "    precision = round(precision_score(labels, pred), 3)\n",
    "    recall = round(recall_score(labels, pred), 3)\n",
    "    \n",
    "    ## Display the result for Model\n",
    "    print(f'{name} -- Accuracy: {accuracy} / Precision: {precision} / Recall: {recall} / Latency: {round((end - start)*1000, 1)}ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR -- Accuracy: 0.775 / Precision: 0.712 / Recall: 0.646 / Latency: 3.9ms\n",
      "SVM -- Accuracy: 0.747 / Precision: 0.672 / Recall: 0.6 / Latency: 4.0ms\n",
      "MLP -- Accuracy: 0.764 / Precision: 0.683 / Recall: 0.662 / Latency: 5.1ms\n",
      "RF -- Accuracy: 0.792 / Precision: 0.85 / Recall: 0.523 / Latency: 4.9ms\n",
      "GB -- Accuracy: 0.815 / Precision: 0.808 / Recall: 0.646 / Latency: 7.6ms\n"
     ]
    }
   ],
   "source": [
    "## Evaluate each model on validation set one by one.\n",
    "\n",
    "for name, model in models.items():\n",
    "    evaluate_model(name, model, val_features, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate best model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest -- Accuracy: 0.782 / Precision: 0.894 / Recall: 0.553 / Latency: 9.8ms\n",
      "Gradient Boosting Tree -- Accuracy: 0.816 / Precision: 0.852 / Recall: 0.684 / Latency: 8.7ms\n"
     ]
    }
   ],
   "source": [
    "## Now we have confusion on 2 models Random Forest and Gradient Boosting\n",
    "## RF has best precision and GBT has best Accuracy and recall\n",
    "## Lest evaluate both on test set and see which perform better\n",
    "evaluate_model('Random Forest', models['RF'], te_features, te_labels)\n",
    "\n",
    "evaluate_model('Gradient Boosting Tree', models['GB'], te_features, te_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Look like Gradient Boosting Tree gives better result so we may end up using GBTree`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
