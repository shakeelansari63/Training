{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd4e8fb6-3606-44c5-ace9-5754272eb258",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05be5b2-71ca-43ea-94eb-267051080228",
   "metadata": {},
   "source": [
    "There are many types of Output Parsers which can parse the LLM output and return a specific schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f06c9b-eae7-4064-a9ca-16097f757041",
   "metadata": {},
   "source": [
    "#### Create an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b5056a8-125c-4456-9495-fbe3416df812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    base_url = 'http://localhost:11434',\n",
    "    model = 'qwen2.5:0.5b'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f9169-5b66-41bb-b318-e52cfe5a2cbb",
   "metadata": {},
   "source": [
    "So if we communicate with this LLM, we will get an object as return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7281bcd4-e623-41e6-b9fc-af5fc8e0fbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Llama is an animal that belongs to the siberian camelids family, commonly known as llamas or mares.\\nEtymology:\\nThe term \"llama\" comes from the Latin \"laima,\" which means \"sheep.\" In the traditional nomadic cultures of central Asia and northern Europe, llamas were often kept by women for their milk and wool.', additional_kwargs={}, response_metadata={'model': 'qwen2.5:0.5b', 'created_at': '2025-03-01T11:59:37.64717774Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1790996301, 'load_duration': 27471278, 'prompt_eval_count': 60, 'prompt_eval_duration': 51000000, 'eval_count': 76, 'eval_duration': 1702000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-452254c7-b373-41b8-baab-b646a88d5d44-0', usage_metadata={'input_tokens': 60, 'output_tokens': 76, 'total_tokens': 136})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = \"\"\"\n",
    "        Provided the word form user, give meaning, example and etymology of the word in 1 sentence each. \n",
    "        Word: {word}\n",
    "    \"\"\",\n",
    "    input_variable=['word']\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke(\"Llama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a66f0f-1fa7-42c7-abda-ed4e4043bbe1",
   "metadata": {},
   "source": [
    "### Str Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af754ab-0ec5-4e00-8a69-721de7bedadc",
   "metadata": {},
   "source": [
    "Now we get lot of information. Now lets say we just need the String Output, we can use StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cef3212d-46c9-4b1e-aa44-b4fdb0abc2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama is the term for an animal that can only be seen from behind and has a white face, with long legs, and a short body.\\n- **Meaning**: An animal that can only be seen from behind.\\n- **Example**: \"The llama in the photograph is seen from behind while running down the street. \"\\n- **Etymology**: The word comes from Old Italian \"laima,\" which means \"white.\" In medieval Latin, this term referred to a white horse or any animal with a white face.\\n\\nLlama\\n- **Meaning**: A type of llama.\\n- **Example**: \"There are many lamas in the area, and some are very old.\"\\n- **Etymology**: The word \"lama\" comes from Old Spanish \"lima,\" which means \"white.\" In medieval Latin, it referred to a white horse or any animal with a white face.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = \"\"\"\n",
    "        Provided the word form user, give meaning, example and etymology of the word in 1 sentence each. \n",
    "        Word: {word}\n",
    "    \"\"\",\n",
    "    input_variable=['word']\n",
    ")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke(\"Llama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f723a1-b6d4-411b-b6c8-c3730f2475d7",
   "metadata": {},
   "source": [
    "### Structured Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46772c6-ab33-422d-971c-6df750b6b901",
   "metadata": {},
   "source": [
    "Now lets say we want the output in a specific structure. We can use Structured Output parser to instruct the LLM to return data in specific structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4d68470-fdbb-4b3b-8979-59c6332b803a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"word\": string  // The word entered by User\n",
      "\t\"meaning\": string  // The meaning of word entered by User\n",
      "\t\"example\": string  // Example for the word entered by user\n",
      "\t\"etymology\": string  // Etymology for the word entered by user\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# Lets generate the schema in which we need the response\n",
    "return_schema = [\n",
    "    ResponseSchema(name=\"word\", description=\"The word entered by User\"),\n",
    "    ResponseSchema(name=\"meaning\", description=\"The meaning of word entered by User\"),\n",
    "    ResponseSchema(name=\"example\", description=\"Example for the word entered by user\"),\n",
    "    ResponseSchema(name=\"etymology\", description=\"Etymology for the word entered by user\"),\n",
    "]\n",
    "\n",
    "# Lets create Output Parser\n",
    "structured_output = StructuredOutputParser.from_response_schemas(return_schema)\n",
    "\n",
    "# When we create a structured output parser, it adds formatting instructions to the prompt\n",
    "format_instruct = structured_output.get_format_instructions()\n",
    "print(format_instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c11c302-876e-4687-b07b-77bf09950780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'etymology': \"Ancient Latin term meaning 'dog-like' or 'small dog', with \"\n",
      "              \"references to its woolly coat, which is why it's also known as \"\n",
      "              'a llama.',\n",
      " 'example': 'The lama was the most famous companion of Jesus Christ and later '\n",
      "            'became known as a symbol of loyalty and devotion.',\n",
      " 'meaning': 'A llama is a small, woolly dog breed that originated in South '\n",
      "            'America and has been domesticated since ancient times.',\n",
      " 'word': 'Llama'}\n"
     ]
    }
   ],
   "source": [
    "# So now we can re-create the prompt template as follow - \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = \"\"\"\n",
    "        Provided the word form user, give meaning, example and etymology of the word in 1 sentence each. \n",
    "        Word: {word}\n",
    "        ----------------\n",
    "        {format_instruct}\n",
    "    \"\"\",\n",
    "    input_variable=['word'],\n",
    "    partial_variables={\"format_instruct\": format_instruct}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | structured_output\n",
    "\n",
    "pprint(chain.invoke(\"Llama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba9a165-8882-4590-b62e-628a858616c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
