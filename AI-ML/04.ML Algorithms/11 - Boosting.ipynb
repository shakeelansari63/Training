{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "Boosting is ensemble method that aggregate number of weak model to make a strong model.  \n",
    "Boosting iteratively builds weak models which learn from previoud model's mistakes making it strong.  \n",
    "\n",
    "## Boosting vs Random Forest\n",
    "Boosting and Random forest are both ensemble methods. The key difference between them is Random forrest build independent models where in Boosting each successor model is dependent on predecessor and learn from predecessor.  \n",
    "## Gradient Boosted Trees\n",
    "Boosting is general umbrella term that cover many varients. Here we will be looking at one specific type called Gradient Boosted Trees.\n",
    "\n",
    "### Training Gradient Boosted Trees\n",
    "Training GBT is time consuming as trees are dependent and created one after other.\n",
    "![GBT Training](img/gbt_train.png)\n",
    "\n",
    "### Testing Gradient Boosted Trees\n",
    "Testing on GBT can be fatser as they can be parallelized. Since Models are already built, test data is passed through each model. Unline _RandomForrest_ where prediction is taken on _majority_, In _GradientBoostedTrees_ Prediction is taken on _weightedvote_ meaning the model which performed better in training gets higher vote.\n",
    "![GBT Testing](img/gbt_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting: Hyperparameters\n",
    "\n",
    "Import [`GradientBoostingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) and [`GradientBoostingRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html) from `sklearn` and explore the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Boosting Algorithm for Classification & Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='deprecated',\n",
      "                           random_state=None, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False) \n",
      "\n",
      " GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
      "                          init=None, learning_rate=0.1, loss='ls', max_depth=3,\n",
      "                          max_features=None, max_leaf_nodes=None,\n",
      "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                          min_samples_leaf=1, min_samples_split=2,\n",
      "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                          n_iter_no_change=None, presort='deprecated',\n",
      "                          random_state=None, subsample=1.0, tol=0.0001,\n",
      "                          validation_fraction=0.1, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "print(GradientBoostingClassifier(), '\\n\\n', GradientBoostingRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major Hyperparemeters in GBT\n",
    "We will be covering only GradientBoostingClassifier here and the hyperparemeter we will consider are - \n",
    "\n",
    "### n_estimators\n",
    "Same as n_estimator in Random forest which control the number of decision trees.\n",
    "\n",
    "### max_depth\n",
    "Same as max_depth in Random Forest which control max depth of each tree\n",
    "\n",
    "### learning_rate\n",
    "Similar to init_learning_rate in Multi-Layer Perceptron. In MLP, learning_rate_init define initial learning rate and learning_rate paremeter define how learning rate change.  \n",
    "  \n",
    "But here, learning_rate correspond to initial learning rate and it stays constant.\n",
    "\n",
    "![Learning Rate](img/lr.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
